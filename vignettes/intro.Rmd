---
title: "Introduction to aquamapsdata"
author: "Markus Skyttner"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to aquamapsdata}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This document describes how to use the `aquamapsdata` R data package to access curated data through a static database assembled from data sourced from http://aquamaps.org

Immediately after installing the package, a run-once action is needed, in order to download and locally create the SQLite database containing all the AquaMaps data.

Approximately 5G disk space is needed locally for the database file. The download is around 1G compressed and therefore a speedy Internet connection is recommended for this initial step.

```{r, eval=FALSE}

# install aquamapsdata from GitHub using devtools

install.packages("devtools") 
library("devtools")
install_gitub("raquamaps/aquamapsdata")

# initial run-once step required to install remote db locally

library(aquamapsdata)
download_db(force = TRUE)

```

Once the database is available locally, it can be queried using a couple of different functions.

# Examples

The following code shows a few ways to query the database through some functions provided in the package

```{r, fig.show='hold', eval=FALSE}

library(aquamapsdata)
library(dplyr)

my_db <- aquamapsdata:::src_sqlite_aquamapsdata()

my_db %>% tbl("nativemaps")
my_db %>% tbl("hcaf")
my_db %>% tbl("hspen")
my_db %>% tbl("occ")
my_db %>% tbl("taxa")

```

## Queries

The `dplyr` package can be used to query the various tables available in the database.

Here is an example of a query for the total record count in one of the tables:

```{r, fig.show='hold', message=FALSE}
library(aquamapsdata)
library(dplyr)

my_db <- aquamapsdata:::src_sqlite_aquamapsdata()

record_count <- 
  my_db %>% tbl("occ") %>% 
  summarize(count = n()) %>% 
  collect %>% 
  .$count

record_count

```

Here is an example of a query for a specific identifier in another table:

```{r, fig.show='hold', message=FALSE}

library(tidyr)

# filter one table for a specific record
taxon_wide <- 
  my_db %>% tbl("taxa") %>% 
  filter(SPECIESID == "Fis-26653") %>%
  collect

# pivot the result for easier display  
taxon_tall <- 
  taxon_wide %>% 
  gather(col_name, col_val)

# display
knitr::kable(taxon_tall)

```

Here is an example of a summary query across several tables:

```{r, fig.show='hold', message=FALSE}
library(dplyr)
library(purrr)

# function which gives the record cound for a given table
ls_count <- function(table) {
  res <- table %>% summarize(count = n()) %>% collect %>% .$count
  title <- as.character(table$ops$x)
  tibble(table = title, record_count = res)
}

# get record counts for all tables
am_counts <- bind_rows(
  my_db %>% tbl("nativemaps") %>% ls_count,
  my_db %>% tbl("hcaf") %>% ls_count,
  my_db %>% tbl("hspen") %>% ls_count,
  my_db %>% tbl("occ") %>% ls_count,
  my_db %>% tbl("taxa") %>% ls_count
)

# display
knitr::kable(am_counts)
```

Here is an example of fuzzy and exact name searches, returning keys with the internal identifiers used in the database. Those keys can be used to retrieve other information, such as environmental envelopes etc.

```{r, fig.show='hold', message=FALSE}

# fuzzy search for "trout OR cod"

keys <- am_name_search_fuzzy("trout OR cod")$key

# exact results for all those keys

hits <- map_df(keys, function(x) am_name_search_exact(key = x))

# we inspect the species list we got 

display <- hits %>% select(key, binomial, rank_family, vernacular)
knitr::kable(display)

```


> What would be the most frequently needed queries? These could be wrapped up and exposed as documented functions.

## Schema

For a table such as the `occ` table, columns are currently named using different styles, as seen in the earlier example. For example a mix of UPPERCASE, snake case delimited lower case words, CamelCasedWord, nocase etc: 

    SPECIESID
    SPECIESID_2016_Default
    IDChange
    Reviewed
    expert_id
    map_beforeafter
    map_seasonal
    picname

> Should column names be harmonized? A "mapping table" could do this.

> If so, what would be a good set of harmonized column names? 

Listing all the columns in all tables of the database and their types gives us a feel for the schema:

```{r, fig.show='hold', message=FALSE}
library(DT)

# for a db table, return a tibble with the columns and their data types
ls_types <- function(table) {
  res <- table %>% head %>% collect %>% lapply(type_sum) %>% unlist
  colname <- names(res)
  title <- as.character(table$ops$x)
  tibble(table = title, col_name = colname, col_type = res, desc = NA)
}

# run the above function on all tables
am_schema <- bind_rows(
  my_db %>% tbl("nativemaps") %>% ls_types,
  my_db %>% tbl("hcaf") %>% ls_types,
  my_db %>% tbl("hspen") %>% ls_types,
  my_db %>% tbl("occ") %>% ls_types,
  my_db %>% tbl("taxa") %>% ls_types
)

datatable(am_schema)

```

Some column names match across tables, do those represent keys that can be used for linking data across the tables?

```{r, fig.show='hold', message=FALSE}

duplicated_colnames <- 
  unique(am_schema$col_name[duplicated(am_schema$col_name)])

am_keys <- 
  am_schema %>% 
  filter(col_name %in% duplicated_colnames) %>% 
  arrange(col_name)

# sometimes the datatypes are different where the column name are equal

knitr::kable(am_keys)

```

> Are those the relevant keys? Do the data types make sense?

```{r, fig.show='hold', eval=FALSE, message=FALSE}

readr::write_csv(am_schema, path = "~/am-schema.csv")
readr::write_csv(am_keys, path = "~/am-keys.csv")

```
